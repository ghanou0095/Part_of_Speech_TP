{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BElI5zDL3Hli"
      },
      "source": [
        "# Lab work 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBfapP71rrvU"
      },
      "source": [
        "## Install nltk\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upvmjw96GQcm",
        "outputId": "0220c55f-c560-4a8c-bacd-5a6311e1ac41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TISbkHZ5rrvk",
        "outputId": "4215a5d0-a1e9-4ddc-9e29-b7ca6385439a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 69,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyMcPA_QrvsM",
        "outputId": "780ef937-4667-4c62-cf68-1174d82efbe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFNLc8Ukvo8X"
      },
      "source": [
        "## import corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAkzV8zjrrvl"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "corpus_root = \"/content/drive/MyDrive/M2 AI/text_mining/corpus/\"\n",
        "corpus = PlaintextCorpusReader(corpus_root,'.*')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U6GnZkytEge"
      },
      "source": [
        "display the files id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWyUBq82sKm5",
        "outputId": "4375f70f-2c70-4538-b859-b6a3d1dae622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tbbts03e01.txt', 'tbbts03e02.txt', 'tbbts03e03.txt', 'tbbts03e04.txt', 'tbbts03e05.txt']\n"
          ]
        }
      ],
      "source": [
        "corpus.fileids()\n",
        "files = corpus.fileids()\n",
        "print(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah0bTA5hxQL5"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# read all the text files\n",
        "speech = [ ]\n",
        "for id in files:\n",
        "  f = corpus.open(id)\n",
        "  speech.append(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSWLdc7ot45i",
        "outputId": "6ed158ee-5b2c-45db-9adb-c57db6574f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I just want you both tknow, when I publish my findings, I won't forget your contributions. \n",
            "- Great. \n",
            "- Thanks. \n",
            "Of course, Ian't mention you in my Nobel acceptance speech, but when I get around to writing my memoirs, you can expect a very effusive footnote and perhaps a signed copy. \n",
            "- We have to tell him. \n",
            "- Tell me what? \n",
            "Damn his Vulcan hearing. \n",
            "You fellows are planning a party for me, aren't you? \n",
            "Okay, Sheldon, sit down. \n",
            "If there's going to be a them I should let you know that I don't care for luau, toga or \" under the sea. \" \n",
            "Yeah, we'll keep that in mind. \n",
            "Look... we neeto talk to you about something that happened at the North Pole. \n",
            "If this is about the night the heat went out, there's nothing to be embarrassed about. \n",
            "It's not about that. \n",
            "And we agreed never speak of it again. \n",
            "So we slept together naked. \n",
            "It was only to keep our core body temperatures from plummeting. \n",
            "He's spking about it. \n",
            "For me, it wasa bonding moment. \n",
            "eldon, you remember the first few weeks we were looking for magnetic monopoles and not finding anything and you were acting like an obnoxious, giant dictator? \n",
            "thought we were going to be gentle with him. \n",
            "That's why I added the \" tator. \" \n",
            "And then when we finally got our first positive data, you were so happy. \n",
            "Oh, yes. \n",
            "In the world of emoticons, I was colon, capital \" D. \" \n",
            "Well, in, what your equipment detected wasn't so much evidence of paradigm-shifting monopoles as it was... static from the electric can opener we were turning on and off. \n",
            "He just went colon, capital \" O. \" \n",
            "- You tampered with my experiment? \n",
            "- We h to. \n",
            "It was the only way to keep you from being such a huge Dickensian. \n",
            "You see that? \n",
            "I add the \" ensian. \" \n",
            "Did Leonard know abouthis? \n",
            "Leonard's my best friend in the world. \n",
            "Surely Leonard didn't know. \n",
            "Actually, it was his idea. \n",
            "Of course it was. \n",
            "The whole planeeks of Leonard. \n",
            "- I missed you so much. \n",
            "- I missed you, too \n",
            "- I couldn't even think of anyone else ile you were gone. \n",
            "- Me, neither. \n",
            "Except for one night when the heat went out. \n",
            "Long story. \n",
            "It's... \n",
            "Don't ask. \n",
            "Leonard. \n",
            "Leonard. \n",
            "Leonard. \n",
            "Do not make a sound. \n",
            "Whispering, \" Do not make a sound... \"... is a sound. \n",
            "Damn his Vulcan hearing. \n",
            "Not a good time, Sheldon. \n",
            "Penny. nny. \n",
            "Penny. \n",
            "Oh, this is ridicuus. \n",
            "What? \n",
            "Hello, Penny I realize you're currently at the mercy of your primitive biological urges, but as you have an enti lifetime of poor decisions ahead of you, may I interrupt this one? \n",
            "* * * * * * * * * \n",
            "Wolowitz has informed me Comeof your grand deception. \n",
            "Do you have anything to say for yourself? \n",
            "Yes, I feel terrible about it. \n",
            "I will never forgive myself, I don't expectou to, either, and I would really apprecie it if you would leave me with Penny fo * * * * * * * \n",
            "Okay, can someone plese me what's going on here? \n",
            "What's going on is I was d to believe I was mang groundbreaking strides in science, when in fact, I was being fed false data at the hands of lowitz, Koothrappali and your furry little boy toy. \n",
            "Is that true? \n",
            "It was the onlway to make him happy. \n",
            "Well, why'd you have to make him happy? \n",
            "Because when he wa't happy, we wanted to kill him. \n",
            "There was even a plan. \n",
            "We were going to throw his Kindle outside, and when he went to get it, lock the do and let him freeze to death. \n",
            "That seems like a bit of an overreaction. \n",
            "No, the overreaction was the plan to tie your limbs to four different sled dog teams and yell, \" Mush. \" \n",
            "Look, we kept the original data. \n",
            "You can still blish the actual results. \n",
            "Yes, but the actual results are unsuccessful and I've already sent an to everyone at the university explaining that \n",
            "I have confirmed string theory and forever changed man's understanding of the universe. \n",
            "Aw, see, yeah, you probably shouldn't have done that. \n",
            "So write another. \n",
            "Set the record straight. \n",
            "It's no big deal. \n",
            "You're right, Leonard. \n",
            "It's not a big deal. \n",
            "All you did was lie to me, destroy my dream and humiliate me in frt of the whole university. \n",
            "That, FYI, was sarcasm. \n",
            "I, in fact, believe it is a big deal. \n",
            "Oh, the poor thing. \n",
            "Yeah, I feel terrible. \n",
            "Aren't you going to go talk to hit. \n",
            "What? \n",
            "Uh, he'll be fine. \n",
            "The guy's a trouper. \n",
            "Come here. \n",
            "No, you're right- - you shouldn't talk to him. \n",
            "I will. \n",
            "Man, I cannot catch a break. \n",
            "Do y want to talk? \n",
            "About what? \n",
            "Being betrayed by my friends? \n",
            "Spending three months at the North Pole for nothing? \n",
            "And I didn't even get to go to Comic-Con! \n",
            "Oh, hon... \n",
            "* * * * * * \n",
            "That'for when I'm sick. \n",
            "Sad is not sick. \n",
            "Sorry. \n",
            "I don't know your sad song. \n",
            "I don't have a sad song. \n",
            "I'm not child. \n",
            "Well, you know, I do understand what you're going through. \n",
            "Really? \n",
            "Did you just have the Nobel Prize in Waitressing stolen from you? \n",
            "Well, no, but when I was a senior in high school, one of my friends heard I was going to be named head cheerlead. \n",
            "Oh, I was so excit. \n",
            "My mom even made me a celebration pie. \n",
            "Then they named stupid Valerie Mobacher head cheerleader. \n",
            "Big ol' slutbag. \n",
            "Are you saying that you think a \" celebtion pie \" is even remotely comparable to a Nobel Prize? \n",
            "Well, they're pretty tasty. \n",
            "And on a different, but not unrelated topic, based on your current efforts to buoy my spirits, do you truly believe that you were ever fit be a cheer leader? \n",
            "Look, Sheldon, I juston't think that the guys and Leonard really meant to hurt you. \n",
            "You know? \n",
            "They just told an unfortunateie to deal with a difficult situation. \n",
            "Okay, you ow what it's like? \n",
            "Remember that scene in the new Star Trek moe when Kirk has to take over the ship, so he tells Spock all that stuff he knew wasn't true, like saying Spock didn't care his mom died? \n",
            "I missed Comic \n",
            "- Con and the new Star Trek movie! \n",
            "- I like the new look. \n",
            "- Thanks. \n",
            "I call it \" the Clooney. \" \n",
            "I call it \" the Mario and Luigi but whatever. \n",
            "Hey, how's eldon doing? \n",
            "Well, he came out of his room this rning wearing with the Force, so I'd and trisay, \" a ttle better. \n",
            "\" th If I may abruptly change the sject, did you and Penny finally... you know. \n",
            "- Howard... \n",
            "- Personally, I d't care, but my genitals wanted me to ask. \n",
            "Welltell your genitals what I do with Penny is none of their business. \n",
            "He says they didn't do it. \n",
            "Sheldon, over here. \n",
            "What are you doing? \n",
            "I feel bad for the guy. \n",
            "Shdon, why are you sitting by yourself? \n",
            "Because I am without friends. \n",
            "Like the proveial cheese, I stand alone. \n",
            "Evenhile seated. \n",
            "Come on. \n",
            "We said we we sorry. \n",
            "It's going to take more than an \" I'm sorry \" \n",
            "and a store-bought apology pie from Penny to make up for what y've done to me. \n",
            "Hey, Cooper. \n",
            "Read your retraction email. \n",
            "Way to destroy your reputation. \n",
            "You see? \n",
            "People have been pointing and laughing at me all morning. \n",
            "That's not true. \n",
            "People have been pointing and laughing at you your whole life. \n",
            "All right, I've had enough. \n",
            "Attention, everyone. \n",
            "I'm Dr. Sheldon Cooper. \n",
            "As many of you in the physics department might know, my career trajectory has taken a minor tour. \n",
            "Off a cliff. \n",
            "My credibility mayhave bee \n",
            "Completely wrecked. \n",
            "But I would like to remind you that in science, there's no such thing as failure. \n",
            "There once was a man who referr to his prediction of a cosmological constant as the single \" biggest blunder \" of his career. \n",
            "That man's name was- - surprise, surprise- \n",
            "- Albert Einstein. \n",
            "Yeah, but research into Dark Energy proved that Einstein's cosmological constant was actually right all along, so you're - surprise, surprise--a loser. \n",
            "Oh, you think you're sclever. \n",
            "Well, let me justell you, while I do not currently have a scathing retort, you check your periodically for a doozy. \n",
            "So much for our friendship with Sheldon. \n",
            "* * * * * \n",
            "Listen, since we got, you know, interrupted last night, \n",
            "I didn't have a chance to give you this. \n",
            "Oh, Leonard, you shouldn't have. \n",
            "Oh, boy! \n",
            "What is it? \n",
            "It's a snowflake. \n",
            "From the North Pole. \n",
            "Are you serious? \n",
            "It'll last forever. \n",
            "I preserved in a one percent solution of polyvinyl acetal resin. \n",
            "Oh, my God. \n",
            "That's the mo romantic thing anyone's ever said to me that I didn't understand. \n",
            "It's actually a pretty simple process. \n",
            "You see, cyanoacrylates are monomers which polymerize on- - \n",
            "Red alert, Leonard. \n",
            "Sheldon ran away. \n",
            "Man, I cannot catch a break. \n",
            "So, how do you know he ranway? \n",
            "Well, he's not answering his phone, he handed in his resignation at the university and he sent me a text that said, \" I'm nning away. \" \n",
            "Okay, well, thanks for letting me know. \n",
            "Well, Leonard, aren't you going to do something? \n",
            "Of course I'm going to do something. \n",
            "Uh, Howard, you checkthe comic book Raj, go to the Thai restaurant. \n",
            "I'll stay here with Penny in her apartment. \n",
            "Oh, damn it. * * * * * \n",
            "A break cannot be caught. \n",
            "Hi, Mrs.Cooper. \n",
            "is? \n",
            "Sheldon went home to Texas \n",
            "Yeah, no, I know he resigned. \n",
            "Yes... * * * * * * * \n",
            "No, no, no. \n",
            "You-you're right. \n",
            "Someone needs to come talk to him. \n",
            "Don't worry, I'll ke care of it. \n",
            "Yeah. \n",
            "All right. \n",
            "New plan. \n",
            "Howard, you and Raj go to Texas. \n",
            "I'll stay here with Penny in her apartment. \n",
            "Well, you're not gonna go with them? \n",
            "Well, you know, I gave you the snowflake and we were kissing and... \n",
            "Oh, come o I don't want to go to Texas! \n",
            "Oh, right, and I do? \n",
            "My people already crossed a desert once. \n",
            "We're done. \n",
            "Trust me, you'll be fine. \n",
            "See ya. \n",
            "Well, wait a second, Leonard, come on, how can you not go? \n",
            "He's your best friend. \n",
            "- Yeah, but I already saw him naked. \n",
            "Just come here. \n",
            "- No. \n",
            "I promise I will be he when you get back. \n",
            "Just go help Sheldon. \n",
            "- Really? \n",
            "- Yeah. \n",
            "We waited a few months. \n",
            "We can wt a few more days. \n",
            "Maybe you can. \n",
            "Go. \n",
            "Boy, you cannot catch a break, canou? \n",
            "Here you go, Shelly. \n",
            "Thks, Mom. \n",
            "Hold your horses, young man. \n",
            "Here in Texas, we pray before we eat. \n",
            "- Aw, Mom. \n",
            "- This is not California, land of the heathen. \n",
            "Gimme. \n",
            "By His hand we are all... fed. \n",
            "Give us, Lord, our daily... bread. \n",
            "- Please know that we are truly... \n",
            "- grateful. \n",
            "- For every cup and every... \n",
            "- plateful. \n",
            "Amen. \n",
            "Now, that wasn't so hard, was it? \n",
            "My objection was based ononsiderations other than difficulty. \n",
            "Whatever. \n",
            "Jesus still loves you. \n",
            "Thank you for carving a smiley face in my grilled cheese sandwich. \n",
            "Oh, I know how to take care of my baby. \n",
            "* * * * * but you can just pretend he's Chinese. \n",
            "ttle thin, So, do you want to talk about what happened with you and yo little friends? \n",
            "They're not my friends. \n",
            "All right. \n",
            "If you recall, when you were little, we s right here at this very spot anwe talked about some of the problems you had gettinalong with the neighbor kids. \n",
            "That was different. \n",
            "They were threatened by my telligence and too stupid to know that's why they hated me. \n",
            "Oh baby, they knew very well why they hatedou. \n",
            "I can't believe you bought a red cowboy hat. \n",
            "Hello? \n",
            "I'm wearing a red turtleneck \n",
            "Plus, it was the only boys' large they had. \n",
            "I'm sorry, this does not look like Texas. \n",
            "Where's the tumbleweeds? \n",
            "Whe's the saloons? \n",
            "Saloons? \n",
            "Yeah, like in the movies I saw growing up in India. \n",
            "You know, uh, Four for Texas, Yellow Rose of Tex. \n",
            "This neighborhood is more Texas Chainsaw \n",
            "I was really hoping to see a cattle drive. \n",
            "What can I tell you? \n",
            "They probably have steaks on sale at that big-ass Costco over there. \n",
            "Will you please take * * \n",
            "No, I want to blend in. \n",
            "To what? \n",
            "Toy Story? \n",
            "- Hi, boys. \n",
            "- Howdy, ma'am. \n",
            "Howdy. \n",
            "You got here qck. \n",
            "- We took the red-eye. \n",
            "- Well, come on in. \n",
            "Thank you kindly. \n",
            "Can... \n",
            "Can I get you something to drink? \n",
            "Uh, no, thank you. \n",
            "If y'all don't mind, I got a hankerin' for a Lone Star beer. \n",
            "There's no alcohol in this usehold. \n",
            "Stop talking like that and lose the hat. \n",
            "I'll take a diet Y-Hoo if you have it. \n",
            "You'll take a Coke. \n",
            "What about you? \n",
            "Radge, isn't it? \n",
            "Oh, you still having trouble talking to the ladies? \n",
            "Because, you know, at o church, we have a woman who's an amazing healer. \n",
            "Mostly she does, uh, crutch and wheelchair people, but I bet she'd be willing to te a shot at whatever Third World demon is running around inse of you. \n",
            "Uh, if you don't mind, Mrs.Cooper, there's a 3 : 05 nonstopback to Los Angele and you have no idea how much I want to be on it. \n",
            "A girl? \n",
            "- Uh, yes, ma'am. \n",
            "- Oh, good. \n",
            "I been praying forou. \n",
            "Oh, Sheldon. \n",
            "What are they doing here? \n",
            "- We came to apologize. \n",
            "- Agn. \n",
            "And bring you home. \n",
            "So, why don't you pack up your stuff and we'll head back \n",
            "No, thiss my home now. \n",
            "Thanks to you, my career is over and I will spend the rest of my life here in Texas trying to teach evolution to creatnists. \n",
            "You watch your mouth, Shelly. \n",
            "Everyone's entitled to their opinion. \n",
            "Evolution isn't an opinion, it's fact. \n",
            "And that is your opinion. \n",
            "I forgive you. \n",
            "Let's go home. \n",
            "* * * * * * * \n",
            "How about that I finally caught a break. \n",
            "You know how ty say when friends have sex it can get weird? \n",
            "Sure. \n",
            "Why does it have to get weird? \n",
            "I don't know. \n",
            "I mean, we were friends, and now we're more than friends. \n",
            "We're whatever \" this \" is. \n",
            "But why label it, right? \n",
            "I mean, it is what it is and... \n",
            "- Leonard? \n",
            "- Yeah? \n",
            "- It's weird. \n",
            "- Totally. \n"
          ]
        }
      ],
      "source": [
        "print(speech[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJDDnL5_uMxX"
      },
      "source": [
        "## sentence segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgDsv3kfAH6o"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "for s in speech:\n",
        "  sentences.append(sent_tokenize(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL9QlCs7wAJH"
      },
      "outputs": [],
      "source": [
        "# merge the list of sentences\n",
        "sentences =[ sent for sublist in sentences for sent in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68TT5ImzCfO3",
        "outputId": "a3f79486-5b72-410b-849a-f24c1005d258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "************FIRST 5 SENTENCES ********\n",
            "I just want you both tknow, when I publish my findings, I won't forget your contributions.\n",
            "- Great.\n",
            "- Thanks.\n",
            "Of course, Ian't mention you in my Nobel acceptance speech, but when I get around to writing my memoirs, you can expect a very effusive footnote and perhaps a signed copy.\n",
            "- We have to tell him.\n"
          ]
        }
      ],
      "source": [
        "print(\"************FIRST 5 SENTENCES ********\")\n",
        "for i in range(5):\n",
        "  print(sentences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMEyhwyE1Stj",
        "outputId": "583fb812-5aad-47b3-fb5a-681c094e95e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of sentences in the coropus =  1868\n"
          ]
        }
      ],
      "source": [
        "print(\"number of sentences in the coropus = \",len(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQDirahyuuuy"
      },
      "source": [
        "## segmentation in Words (tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvtzIbvNA1uC"
      },
      "outputs": [],
      "source": [
        "words = [ word_tokenize(sent) for sent in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiSapqgBCzlf",
        "outputId": "16171410-84c2-42aa-abce-9f8bb7030724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "************** FIRST 5 SENTENCES AFTER WORD TOKENIZATION**************\n",
            "['I', 'just', 'want', 'you', 'both', 'tknow', ',', 'when', 'I', 'publish', 'my', 'findings', ',', 'I', 'wo', \"n't\", 'forget', 'your', 'contributions', '.']\n",
            "['-', 'Great', '.']\n",
            "['-', 'Thanks', '.']\n",
            "['Of', 'course', ',', 'Ia', \"n't\", 'mention', 'you', 'in', 'my', 'Nobel', 'acceptance', 'speech', ',', 'but', 'when', 'I', 'get', 'around', 'to', 'writing', 'my', 'memoirs', ',', 'you', 'can', 'expect', 'a', 'very', 'effusive', 'footnote', 'and', 'perhaps', 'a', 'signed', 'copy', '.']\n",
            "['-', 'We', 'have', 'to', 'tell', 'him', '.']\n"
          ]
        }
      ],
      "source": [
        "print(\"************** FIRST 5 SENTENCES AFTER WORD TOKENIZATION**************\")\n",
        "for i in range(5):\n",
        "  print(words[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5sHhJoc3W9c"
      },
      "source": [
        "## Part Of Speech tagging (POS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAgq7llX6AU8"
      },
      "outputs": [],
      "source": [
        "POSs = [nltk.pos_tag(sent) for sent in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV8uVrM98DsQ",
        "outputId": "376f892b-b818-467f-a425-40b60ddaa448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('just', 'RB'), ('want', 'VBP'), ('you', 'PRP'), ('both', 'DT'), ('tknow', 'VBP'), (',', ','), ('when', 'WRB'), ('I', 'PRP'), ('publish', 'VBP'), ('my', 'PRP$'), ('findings', 'NNS'), (',', ','), ('I', 'PRP'), ('wo', 'MD'), (\"n't\", 'RB'), ('forget', 'VB'), ('your', 'PRP$'), ('contributions', 'NNS'), ('.', '.')]\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "print(POSs[0])\n",
        "print(len(POSs[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd5OA_W4YdRV"
      },
      "source": [
        "### Counting the most common lexical categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hjId-SaX_e9",
        "outputId": "4e0154e7-f09c-42be-e251-9c48004aee5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['PRP', 'RB', 'VBP', 'PRP', 'DT', 'VBP', ',', 'WRB', 'PRP', 'VBP', 'PRP$', 'NNS', ',', 'PRP', 'MD', 'RB', 'VB', 'PRP$', 'NNS', '.']\n"
          ]
        }
      ],
      "source": [
        "# get the list of tags\n",
        "pos_tags = [tag[1] for sent in POSs for tag in sent]\n",
        "print(pos_tags[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua4uoIgFZahK",
        "outputId": "a865fcbe-0f61-42e7-e1d0-41a7ce22b02c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('.', 1825), ('NN', 1698), ('PRP', 1681), ('IN', 1012), ('DT', 951), ('NNP', 902), ('RB', 834)]\n"
          ]
        }
      ],
      "source": [
        "fd = nltk.FreqDist(pos_tags)\n",
        "print(fd.most_common(7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ELiMs3aCfy"
      },
      "source": [
        "when excluding the punctuation, we can see that nouns are the most common lexical category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjF-NGEUqR2R"
      },
      "source": [
        "### Most common used verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scJoFIkAqbdN",
        "outputId": "013af15e-dca9-4f89-a067-d01a0b917a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['want', 'tknow', 'publish', 'forget', 'Ia', 'get', 'writing', 'expect', 'signed', 'have', 'tell', 'Tell', 'are', 'planning', 'are', 'sit', \"'s\", 'going', 'be', 'let']\n"
          ]
        }
      ],
      "source": [
        "verbs = [tag[0] for sent in POSs for tag in sent if tag[1].startswith('VB')]\n",
        "print(verbs[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqcxA7bTrEoX",
        "outputId": "b3a8d6e1-0a86-4fab-c598-7dec1e1b402d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(\"'s\", 153), ('do', 104), ('is', 96), ('have', 91), (\"'m\", 88), ('was', 87), (\"'re\", 84), ('are', 72), ('know', 69), ('be', 63)]\n"
          ]
        }
      ],
      "source": [
        "fd = nltk.FreqDist(verbs)\n",
        "print(fd.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap7CSTDDcIp7"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD3r6z87rVCi"
      },
      "source": [
        "lemmatization of verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLyMY13HkKDT",
        "outputId": "4d592cf3-6b1d-4d0b-9ec8-bd2e30bf57e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "want                want                \n",
            "tknow               tknow               \n",
            "publish             publish             \n",
            "forget              forget              \n",
            "Ia                  Ia                  \n",
            "get                 get                 \n",
            "writing             write               \n",
            "expect              expect              \n",
            "signed              sign                \n",
            "have                have                \n",
            "tell                tell                \n",
            "Tell                Tell                \n",
            "are                 be                  \n",
            "planning            plan                \n",
            "are                 be                  \n",
            "sit                 sit                 \n",
            "'s                  's                  \n",
            "going               go                  \n",
            "be                  be                  \n",
            "let                 let                 \n",
            "know                know                \n",
            "do                  do                  \n",
            "care                care                \n",
            "keep                keep                \n",
            "neeto               neeto               \n",
            "talk                talk                \n",
            "happened            happen              \n",
            "is                  be                  \n",
            "went                go                  \n",
            "'s                  's                  \n",
            "be                  be                  \n",
            "embarrassed         embarrass           \n",
            "'s                  's                  \n",
            "agreed              agree               \n",
            "slept               sleep               \n",
            "naked               naked               \n",
            "was                 be                  \n",
            "keep                keep                \n",
            "plummeting          plummet             \n",
            "'s                  's                  \n",
            "spking              spking              \n",
            "wasa                wasa                \n",
            "bonding             bond                \n",
            "remember            remember            \n",
            "were                be                  \n",
            "looking             look                \n",
            "finding             find                \n",
            "were                be                  \n",
            "acting              act                 \n",
            "thought             think               \n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import  WordNetLemmatizer\n",
        "wordnet_lammatizer = WordNetLemmatizer()\n",
        "lemmatized_verbs = [wordnet_lammatizer.lemmatize(v,pos='v') for v in verbs]\n",
        "for i in range(50):\n",
        "  print(\"{0:20}{1:20}\".format(verbs[i],lemmatized_verbs[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd6SiRmn0ODM"
      },
      "source": [
        "## Using spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qIOs86UEGEQ",
        "outputId": "b5523a95-8d0c-4ca9-8f23-c407be47277e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (51.3.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k144Fcmt0UCy"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpIo8SBrTsRi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}